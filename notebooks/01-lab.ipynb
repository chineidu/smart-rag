{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7cad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Coroutine, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "pl.Config.set_tbl_rows(n=200)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)\n",
    "\n",
    "\n",
    "# Prevents ruff from removing the unused module import\n",
    "_ = [Coroutine, Path, Literal, json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da164420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/Projects/smart-rag\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=1)\n",
    "\n",
    "from src.config import app_settings  # noqa: E402 # type: ignore\n",
    "from src.utilities.model_config import RemoteModel  # noqa: E402 # type: ignore\n",
    "\n",
    "settings = app_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec396d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527e9ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Why don’t scientists trust atoms?  \n",
      "\n",
      "Because they make up everything.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "remote_llm = ChatOpenAI(\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),  # type: ignore\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    seed=1,\n",
    "    model=RemoteModel.GPT_OSS_120B,\n",
    ")\n",
    "\n",
    "\n",
    "# Test the LLMs\n",
    "response = remote_llm.invoke(\"Tell me a very short joke.\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "593ca6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why don’t scientists trust atoms?  \\n\\nBecause they make up everything.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">115</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'accepted_prediction_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'rejected_prediction_tokens'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'image_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'video_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cost'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5444e-05</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'is_byok'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cost_details'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'upstream_inference_cost'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'upstream_inference_prompt_cost'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.4e-06</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'upstream_inference_completions_cost'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.2e-06</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_provider'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'openai/gpt-oss-120b'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gen-1767111263-JATNAGkTVmM4Z0cwYihw'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'lc_run--11fea449-66f4-4fe0-8cbb-783f1bbab7ec-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">115</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_token_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'audio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cache_read'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_token_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Why don’t scientists trust atoms?  \\n\\nBecause they make up everything.'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m41\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m74\u001b[0m,\n",
       "            \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m115\u001b[0m,\n",
       "            \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'accepted_prediction_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[32m'audio_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m19\u001b[0m,\n",
       "                \u001b[32m'rejected_prediction_tokens'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[32m'image_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'video_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'cost'\u001b[0m: \u001b[1;36m1.5444e-05\u001b[0m,\n",
       "            \u001b[32m'is_byok'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "            \u001b[32m'cost_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'upstream_inference_cost'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[32m'upstream_inference_prompt_cost'\u001b[0m: \u001b[1;36m7.4e-06\u001b[0m,\n",
       "                \u001b[32m'upstream_inference_completions_cost'\u001b[0m: \u001b[1;36m8.2e-06\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_provider'\u001b[0m: \u001b[32m'openai'\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'openai/gpt-oss-120b'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'id'\u001b[0m: \u001b[32m'gen-1767111263-JATNAGkTVmM4Z0cwYihw'\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'lc_run--11fea449-66f4-4fe0-8cbb-783f1bbab7ec-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m74\u001b[0m,\n",
       "        \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m41\u001b[0m,\n",
       "        \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m115\u001b[0m,\n",
       "        \u001b[32m'input_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'cache_read'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'output_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning'\u001b[0m: \u001b[1;36m19\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b92e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import uvloop\n",
    "\n",
    "# Use Uvloop's implementation (Place this at the entrypoint)\n",
    "asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b06df770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generic\n",
    "\n",
    "from pydantic import BaseModel, ConfigDict, RootModel\n",
    "\n",
    "from src.schemas.types import T\n",
    "\n",
    "\n",
    "class ModelList(BaseModel, Generic[T]):\n",
    "    \"\"\"Generic container for lists of Pydantic BaseModel objects.\n",
    "\n",
    "    This class provides type-safe handling of any BaseModel subclass,\n",
    "    including LangChain Documents, with validation and utility methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    items : list[T]\n",
    "        List of BaseModel instances.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # With Documents\n",
    "    >>> docs = ModelList[Document](items=[\n",
    "    ...     Document(page_content=\"Hello\", metadata={\"source\": \"test\"}),\n",
    "    ...     Document(page_content=\"World\", metadata={\"source\": \"test2\"})\n",
    "    ... ])\n",
    "\n",
    "    >>> # With custom Pydantic models\n",
    "    >>> class User(BaseModel):\n",
    "    ...     name: str\n",
    "    ...     age: int\n",
    "    >>>\n",
    "    >>> users = ModelList[User](items=[\n",
    "    ...     User(name=\"Alice\", age=30),\n",
    "    ...     User(name=\"Bob\", age=25)\n",
    "    ... ])\n",
    "    \"\"\"\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    items: list[T]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of items.\"\"\"\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, index: int) -> T:\n",
    "        \"\"\"Get item by index.\"\"\"\n",
    "        return self.items[index]\n",
    "\n",
    "    def append(self, item: T) -> None:\n",
    "        \"\"\"Add an item to the list.\"\"\"\n",
    "        self.items.append(item)\n",
    "\n",
    "    def extend(self, items: list[T]) -> None:\n",
    "        \"\"\"Extend the list with multiple items.\"\"\"\n",
    "        self.items.extend(items)\n",
    "\n",
    "    def to_dicts(self) -> list[dict[str, Any]]:\n",
    "        \"\"\"Convert all items to dictionaries.\"\"\"\n",
    "        return [\n",
    "            item.model_dump() if hasattr(item, \"model_dump\") else dict(item)\n",
    "            for item in self.items\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def from_dicts(\n",
    "        cls, data: list[dict[str, Any]], model_class: type[T]\n",
    "    ) -> \"ModelList[T]\":\n",
    "        \"\"\"Create a ModelList from a list of dictionaries.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : list[dict[str, Any]]\n",
    "            List of dictionaries to convert.\n",
    "        model_class : type[T]\n",
    "            The Pydantic model class to instantiate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ModelList[T]\n",
    "            A new ModelList instance.\n",
    "        \"\"\"\n",
    "        items = [model_class(**item) for item in data]\n",
    "        return cls(items=items)\n",
    "\n",
    "\n",
    "class ModelListUpdated(RootModel[list[T]], Generic[T]):\n",
    "    \"\"\"Generic container for lists of Pydantic BaseModel objects.\n",
    "\n",
    "    This class provides type-safe handling of any BaseModel subclass,\n",
    "    including LangChain Documents, with validation and utility methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    items : list[T]\n",
    "        List of BaseModel instances.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # With Documents\n",
    "    >>> docs = ModelList[Document](items=[\n",
    "    ...     Document(page_content=\"Hello\", metadata={\"source\": \"test\"}),\n",
    "    ...     Document(page_content=\"World\", metadata={\"source\": \"test2\"})\n",
    "    ... ])\n",
    "\n",
    "    >>> # With custom Pydantic models\n",
    "    >>> class User(BaseModel):\n",
    "    ...     name: str\n",
    "    ...     age: int\n",
    "    >>>\n",
    "    >>> users = ModelList[User](items=[\n",
    "    ...     User(name=\"Alice\", age=30),\n",
    "    ...     User(name=\"Bob\", age=25)\n",
    "    ... ])\n",
    "    \"\"\"\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    root: list[T]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of items.\"\"\"\n",
    "        return len(self.root)\n",
    "\n",
    "    def __getitem__(self, index: int) -> T:\n",
    "        \"\"\"Get item by index.\"\"\"\n",
    "        return self.root[index]\n",
    "\n",
    "    def append(self, item: T) -> None:\n",
    "        \"\"\"Add an item to the list.\"\"\"\n",
    "        self.root.append(item)\n",
    "\n",
    "    def extend(self, items: list[T]) -> None:\n",
    "        \"\"\"Extend the list with multiple items.\"\"\"\n",
    "        self.root.extend(items)\n",
    "\n",
    "    def to_dicts(self) -> list[dict[str, Any]]:\n",
    "        \"\"\"Convert all items to dictionaries.\"\"\"\n",
    "        return self.model_dump()\n",
    "\n",
    "    @classmethod\n",
    "    def from_dicts(\n",
    "        cls, data: list[dict[str, Any]], model_class: type[T]\n",
    "    ) -> \"ModelList[T]\":\n",
    "        \"\"\"Create a ModelList from a list of dictionaries.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : list[dict[str, Any]]\n",
    "            List of dictionaries to convert.\n",
    "        model_class : type[T]\n",
    "            The Pydantic model class to instantiate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ModelList[T]\n",
    "            A new ModelList instance.\n",
    "        \"\"\"\n",
    "        items = [model_class(**item) for item in data]\n",
    "        return cls(root=items)\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d60759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelList[User](items=[User(name='Alice', age=30), User(name='Bob', age=25)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_1 = User(name=\"Alice\", age=30)\n",
    "user_2 = User(name=\"Bob\", age=25)\n",
    "my_obj = ModelList[User](items=[user_1, user_2])\n",
    "my_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cab71270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelList[User](items=[User(name='Alice', age=30), User(name='Bob', age=25), User(name='Charlie', age=28)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.append(User(name=\"Charlie\", age=28))\n",
    "\n",
    "my_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b087c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'name': 'Alice', 'age': 30}], [{'name': 'Alice', 'age': 30}])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_obj = ModelListUpdated[User](root=[user_1])\n",
    "other_obj.model_dump(), other_obj.to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09f9d92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items': [{'name': 'Alice', 'age': 30},\n",
       "  {'name': 'Bob', 'age': 25},\n",
       "  {'name': 'Charlie', 'age': 28}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81ec8d2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m a = [{\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mBob\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m25\u001b[39m}]\n\u001b[32m      2\u001b[39m b = [{\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mAlice\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m30\u001b[39m}]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfromkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "a = [{\"name\": \"Bob\", \"age\": 25}]\n",
    "b = [{\"name\": \"Alice\", \"age\": 30}]\n",
    "list(dict.fromkeys(a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9c79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Bob', 'age': 25}, {'name': 'Alice', 'age': 30}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-rag (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
