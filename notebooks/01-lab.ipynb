{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7cad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from typing import Any, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "pl.Config.set_tbl_rows(n=200)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba7e078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Smart-RAG', 'version': '1.0'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)\n",
    "\n",
    "\n",
    "# Demo (Prevents ruff from removing the unused module import)\n",
    "name: Any\n",
    "category: Literal[\"A\", \"B\", \"C\"]\n",
    "json.loads('{\"name\": \"Smart-RAG\", \"version\": \"1.0\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da164420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/Projects/smart-rag\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=1)\n",
    "\n",
    "from src.config import app_config, app_settings  # noqa: E402\n",
    "from src.utilities.model_config import RemoteModel  # noqa: E402\n",
    "\n",
    "settings = app_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23cb4600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 19:53:33 - vectorstores - [INFO] - AI news filepath: /Users/mac/Desktop/Projects/smart-rag/data/ai_news\n",
      "2025-10-25 19:53:34 - vectorstores - [INFO] - Loaded 43 documents from 5 filepaths.\n",
      "2025-10-25 19:53:36 - vectorstores - [INFO] - Qdrant vector store set up with collection 'ai_news' and vector size 768\n",
      "2025-10-25 19:53:40 - vectorstores - [INFO] - Embedded and stored 168 documents.\n",
      "2025-10-25 19:53:40 - vectorstores - [INFO] - AI news vector store setup complete.\n",
      "2025-10-25 19:53:44 - vectorstores - [INFO] - Loaded 174 documents from 7 filepaths.\n",
      "2025-10-25 19:53:44 - vectorstores - [INFO] - Qdrant vector store set up with collection 'football_news' and vector size 768\n",
      "2025-10-25 19:53:54 - vectorstores - [INFO] - Embedded and stored 567 documents.\n",
      "2025-10-25 19:53:54 - vectorstores - [INFO] - Football news vector store setup complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_qdrant.qdrant.QdrantVectorStore at 0x1502b20f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utilities.vectorstores import ai_vectorstore\n",
    "\n",
    "ai_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1b653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7cfc43",
   "metadata": {},
   "source": [
    "# Adaptive RAG\n",
    "\n",
    "Adaptive RAG is an advanced retrieval-augmented generation (RAG) architecture that intelligently combines **traditional RAG techniques** with **self-reflection** and **external tool usage** to enhance the quality and reliability of generated answers.\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/65nZVgN9/image.png)](https://postimg.cc/ZCY04fRg)\n",
    "\n",
    "[Source: LangChain](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Breakdown of the above diagram\n",
    "\n",
    "#### 1.) Query Analysis (Red Box)\n",
    "\n",
    "- The user's Question first goes to a Query Analysis step (likely an LLM prompt or classifier).\n",
    "- It determines if the question is [related to index] (the internal knowledge base) or [unrelated to index] (requiring external tools).\n",
    "\n",
    "#### 2.) RAG + Self-Reflection (Top Dashed Box)\n",
    "\n",
    "- If related to the index, the query proceeds through the RAG workflow.\n",
    "- Retrieve & Grade: The system fetches documents (Retrieve Node), and then an LLM agent grades their relevance.\n",
    "- Decision Loop:\n",
    "  - If the documents are relevant, it proceeds to Generate (Node) and then checks for Hallucinations?\n",
    "  - If the answer is free of hallucinations and Answers question? successfully, the process stops with the final Answer.\n",
    "  - If the documents are not relevant or the generated answer fails the self-reflection checks, the question is sent to the Re-write question (Node) and the process loops back to Retrieve. This allows the agent to iteratively improve its search query.\n",
    "\n",
    "#### 3.) Tool Use (Bottom Green Path)\n",
    "\n",
    "- If the original Query Analysis determined the question was [unrelated to index], it activates the Web search tool, generates the answer using the web results, and provides the Answer w/ web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec396d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "remote_llm = ChatOpenAI(\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),  # type: ignore\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    model=RemoteModel.GEMINI_2_0_FLASH_001,\n",
    ")\n",
    "\n",
    "\n",
    "# Test the LLMs\n",
    "response = remote_llm.invoke(\"Tell me a very short joke.\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1cfc2c",
   "metadata": {},
   "source": [
    "### Load documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "\n",
    "def load_pdf_doc(filepath: str, engine: str = \"pypdfloader\") -> list[Document]:\n",
    "    \"\"\"This is used to load a single document.\"\"\"\n",
    "    if engine == \"pypdfloader\":\n",
    "        loader = PyPDFLoader(filepath)\n",
    "    docs: list[Document] = loader.load()\n",
    "    return docs\n",
    "\n",
    "\n",
    "# Load a single doc\n",
    "fp: str = \"../data/ai_news/Inside_San_Francisco’s_AI_school.pdf\"\n",
    "docs = load_pdf_doc(filepath=fp)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple docs\n",
    "ai_filepaths: list[str] = glob(\"../data/ai_news/*.pdf\")\n",
    "docs_ai = [doc for fp in ai_filepaths for doc in load_pdf_doc(filepath=fp)]\n",
    "print(len(docs_ai))\n",
    "\n",
    "football_filepaths: list[str] = glob(\"../data/football_news/*.pdf\")\n",
    "docs_football = [doc for fp in football_filepaths for doc in load_pdf_doc(filepath=fp)]\n",
    "print(len(docs_football))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ai[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4762883",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(docs_ai[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20eb36c",
   "metadata": {},
   "source": [
    "### Set up document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, List\n",
    "\n",
    "import together\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.utils import convert_to_secret_str\n",
    "from pydantic import (\n",
    "    BaseModel,\n",
    "    ConfigDict,\n",
    "    Field,\n",
    "    SecretStr,\n",
    "    model_validator,\n",
    ")\n",
    "\n",
    "\n",
    "def set_together_api(value: str | None = None) -> SecretStr:\n",
    "    \"\"\"Set the Together API key\"\"\"\n",
    "    if value is None:\n",
    "        return convert_to_secret_str(os.getenv(\"TOGETHER_API_KEY\", \"\"))\n",
    "    return convert_to_secret_str(value)\n",
    "\n",
    "\n",
    "class TogetherEmbeddings(BaseModel, Embeddings):\n",
    "    \"\"\"Using Field with default_factory for automatic client creation.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    client: together.Together = Field(default_factory=together.Together)\n",
    "    together_api_key: SecretStr = Field(default_factory=lambda: set_together_api)\n",
    "    model: str = Field(default=\"togethercomputer/m2-bert-80M-32k-retrieval\")\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_environment(cls, values: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Set up the Together API key and client before model instantiation.\"\"\"\n",
    "        # Handle API key setup\n",
    "        api_key = values.get(\"together_api_key\") or os.getenv(\"TOGETHER_API_KEY\", \"\")\n",
    "        if isinstance(api_key, str):\n",
    "            api_key = set_together_api(api_key)\n",
    "        values[\"together_api_key\"] = api_key\n",
    "        values[\"client\"] = together.Together()\n",
    "\n",
    "        # Set global API key\n",
    "        together.api_key = api_key.get_secret_value()\n",
    "\n",
    "        return values\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        return [\n",
    "            i.embedding\n",
    "            for i in self.client.embeddings.create(input=texts, model=self.model).data\n",
    "        ]  # type: ignore\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        return self.embed_documents([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a known working Together AI model\n",
    "embeddings = TogetherEmbeddings(\n",
    "    model=\"BAAI/bge-base-en-v1.5\",  # Using known working model\n",
    "    together_api_key=settings.TOGETHER_API_KEY,\n",
    ")\n",
    "\n",
    "# Test the embedding\n",
    "try:\n",
    "    test_text = \"This is a test embedding\"\n",
    "    result = embeddings.embed_query(test_text)\n",
    "    console.print(f\"✅ Embedding successful! Dimension: {len(result)}\", style=\"success\")\n",
    "    console.print(f\"First 5 values: {result[:5]}\", style=\"info\")\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"❌ Embedding failed: {e}\", style=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "vector_size = len(embeddings.embed_query(\"sample text\"))\n",
    "collection_name_ai = \"ai_news\"\n",
    "\n",
    "if not client.collection_exists(collection_name_ai):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name_ai,\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "    )\n",
    "vectorstore_ai = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name_ai,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "collection_name_football = \"football_news\"\n",
    "\n",
    "if not client.collection_exists(collection_name_football):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name_football,\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "    )\n",
    "vectorstore_football = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name_football,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1771c29",
   "metadata": {},
   "source": [
    "### Split Documents Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f13f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_ai = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # chunk size (characters)\n",
    "    chunk_overlap=50,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits_ai = text_splitter_ai.split_documents(docs_ai)\n",
    "\n",
    "print(f\"Split into {len(all_splits_ai)} sub-documents.\")\n",
    "\n",
    "\n",
    "text_splitter_football = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # chunk size (characters)\n",
    "    chunk_overlap=50,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits_football = text_splitter_football.split_documents(docs_football)\n",
    "\n",
    "print(f\"Split into {len(all_splits_football)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747209e6",
   "metadata": {},
   "source": [
    "### Embed The Document Chunks And Add to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids: list[str] = vectorstore_ai.add_documents(documents=all_splits_ai)\n",
    "print(document_ids[:3])\n",
    "\n",
    "document_ids: list[str] = vectorstore_football.add_documents(\n",
    "    documents=all_splits_football\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede46cd8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Alternative Method\n",
    "\n",
    "- Fewer lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN: bool = False\n",
    "\n",
    "if RUN:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # chunk size (characters)\n",
    "        chunk_overlap=100,  # chunk overlap (characters)\n",
    "        add_start_index=True,  # track index in original document\n",
    "    )\n",
    "\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    vector_store = QdrantVectorStore.from_documents(\n",
    "        documents=all_splits,\n",
    "        embedding=embeddings,\n",
    "        location=\":memory:\",  # for in-memory Qdrant instance\n",
    "        # OR specify url and api_key for external Qdrant instance\n",
    "        # url=\"http://localhost:6333\",  # assuming local Qdrant server is running\n",
    "        # api_key=\"your_qdrant_api_key\",  # if needed (for cloud instances)\n",
    "        collection_name=\"test\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model: RemoteModel = RemoteModel(\n",
    "    app_config.llm_model_config.classifier_model.model_name\n",
    ")\n",
    "\n",
    "\n",
    "# from src.utilities.vectorstores import VectorStoreSetup\n",
    "\n",
    "# my_vectorstore_setup = VectorStoreSetup(\n",
    "#     collection_name=\"test_collection\",\n",
    "#     filepaths=ai_filepaths,\n",
    "#     embeddings=embeddings,\n",
    "#     client=client,\n",
    "# )\n",
    "# my_vectorstore_setup.setup()\n",
    "# vectorstore_instance = my_vectorstore_setup.embed_and_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0dd18",
   "metadata": {},
   "source": [
    "### Test Retrieval from Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2fa81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query: str = \"Has Nvidia broken any laws?\"\n",
    "\n",
    "retrieved_docs = vectorstore_ai.similarity_search(query, k=2)\n",
    "formatted_docs: str = \"\\n\\n\".join(\n",
    "    (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\") for doc in retrieved_docs\n",
    ")\n",
    "\n",
    "console.print(formatted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74bbc4",
   "metadata": {},
   "source": [
    "### Convert Retriever To A Tool\n",
    "\n",
    "- I eventually did NOT use this approach in the final implementation, but it's good to know how to do it.\n",
    "- This is because when the agent uses the tool, it returns the output as a string, which makes it difficult to pass the retrieved documents to the LLM call node for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ca6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retriever_ai_tool(query: str) -> tuple[str, list[Document]]:\n",
    "    \"\"\"Retrieve information related to AI news to help answer a query.\n",
    "    AI news include ANY info relating to: nvidia, openai, google, chinese tech news, etc.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    query: str\n",
    "        The search query to retrieve relevant documents.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple[str, list[Document]]\n",
    "        A tuple containing formatted string of retrieved documents and the list of Document objects.\n",
    "    \"\"\"\n",
    "    retrieved_docs = vectorstore_ai.similarity_search(query, k=2)\n",
    "    formatted_docs: str = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return formatted_docs, retrieved_docs\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retriever_football_tool(query: str) -> tuple[str, list[Document]]:\n",
    "    \"\"\"Retrieve information related to football news to help answer a query.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    query: str\n",
    "        The search query to retrieve relevant documents.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple[str, list[Document]]\n",
    "        A tuple containing formatted string of retrieved documents and the list of Document objects.\n",
    "    \"\"\"\n",
    "    retrieved_docs = vectorstore_football.similarity_search(query, k=2)\n",
    "    formatted_docs: str = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return formatted_docs, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = remote_llm.bind_tools([retriever_ai_tool, retriever_football_tool])\n",
    "response = await llm_with_tools.ainvoke(\"Has Nvidia broken any laws?\")\n",
    "\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c54bd3",
   "metadata": {},
   "source": [
    "#### Create Additional Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_search = TavilySearch(\n",
    "    api_key=settings.TAVILY_API_KEY.get_secret_value(),\n",
    "    max_results=2,\n",
    "    topic=\"general\",\n",
    ")\n",
    "search_response = tavily_search.invoke({\"query\": \"Has Nvidia broken any laws?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e1d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_response[\"results\"][0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b50b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format=\"content\")\n",
    "async def search_tool(query: str, max_chars: int = 500) -> str:\n",
    "    \"\"\"Perform a search using TavilySearch tool.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    query: str\n",
    "        The search query.\n",
    "    max_chars: int, default=1000\n",
    "        The maximum number of characters per source to return from the search results.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The formatted search results.\n",
    "    \"\"\"\n",
    "    separator: str = \"\\n\\n\"\n",
    "\n",
    "    tavily_search = TavilySearch(\n",
    "        api_key=settings.TAVILY_API_KEY.get_secret_value(),\n",
    "        max_results=3,\n",
    "        topic=\"general\",\n",
    "    )\n",
    "    search_response = await tavily_search.ainvoke({\"query\": query})\n",
    "    formatted_results: str = \"\\n\\n\".join(\n",
    "        f\"Title: {result['title']}\\nContent: {result['content'][:max_chars]} [truncated]\\nURL: {result['url']}{separator}\"\n",
    "        for result in search_response[\"results\"]\n",
    "    )\n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await search_tool.coroutine(\"who is pope leo?\")\n",
    "console.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_response = await search_tool.coroutine(\"Has Nvidia broken any laws?\")\n",
    "console.print(search_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc25207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# ============================= TYPES ==============================\n",
    "# ==================================================================\n",
    "class YesOrNo(str, Enum):\n",
    "    YES = \"yes\"\n",
    "    NO = \"no\"\n",
    "\n",
    "\n",
    "class DataSource(str, Enum):\n",
    "    VECTORSTORE = \"vectorstore\"\n",
    "    WEBSEARCH = \"websearch\"\n",
    "\n",
    "\n",
    "class VectorSearchType(str, Enum):\n",
    "    FOOTBALL = \"football news\"  # \"(arsenal news | chelsea news | liverpool news)\"\n",
    "    AI = \"ai news\"  # \"(ai news | ai browser | nvidia | openai| tech in china)\"\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# ============================ SCHEMAS =============================\n",
    "# ==================================================================\n",
    "class RouteQuerySchema(BaseModel):\n",
    "    \"\"\"Route query model.\"\"\"\n",
    "\n",
    "    data_source: DataSource = Field(description=\"The data source to use for the query.\")\n",
    "\n",
    "\n",
    "class VectorSearchTypeSchema(BaseModel):\n",
    "    \"\"\"Vector search type model.\"\"\"\n",
    "\n",
    "    vector_search_type: VectorSearchType = Field(\n",
    "        description=\"The vector search type to use for the query.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GradeRetrievalSchema(BaseModel):\n",
    "    \"\"\"Grade retrieval model.\"\"\"\n",
    "\n",
    "    is_relevant: YesOrNo = Field(\n",
    "        description=\"Whether the retrieved documents are relevant to the user query.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GradeResponseSchema(BaseModel):\n",
    "    \"\"\"Grade response model.\"\"\"\n",
    "\n",
    "    is_relevant: YesOrNo = Field(\n",
    "        description=\"Whether the response is relevant to the user query.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class HallucinationSchema(BaseModel):\n",
    "    \"\"\"Check hallucination model.\"\"\"\n",
    "\n",
    "    is_hallucinating: YesOrNo = Field(\n",
    "        description=\"Whether the response contains hallucinations.\"\n",
    "    )\n",
    "\n",
    "\n",
    "topics: list[str] = [_topic.value for _topic in VectorSearchType]\n",
    "valid_output: list[str] = [_typ.value for _typ in YesOrNo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e91b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# ============================ PROMPTS =============================\n",
    "# ==================================================================\n",
    "query_analysis_prompt: str = \"\"\"\n",
    "<SYSTEM>\n",
    "    <ROLE>\n",
    "        You're an expert at determining whether a user query requires information from a vector store or a web search.\n",
    "    </ROLE>\n",
    "    <TOPICS>{topics}</TOPICS>\n",
    "\n",
    "    <GUIDELINES>\n",
    "    - If the query is related to the topics above, choose 'vectorstore'.\n",
    "    - If the query is not covered by the topics above, choose 'websearch'.\n",
    "    - Base your decision solely on the content of the query.\n",
    "    </GUIDELINES>\n",
    "</SYSTEM> \n",
    "\"\"\"\n",
    "\n",
    "retrieval_grading_prompt: str = \"\"\"\n",
    "<SYSTEM>\n",
    "    <ROLE>\n",
    "        You're an expert at determining whether the retrieved documents from a vector store is relevant to the user query.\n",
    "    </ROLE>\n",
    "    <VALID_OUTPUT>{valid_output}</VALID_OUTPUT>\n",
    "\n",
    "    <GUIDELINES>\n",
    "    - If the documents are relevant to the user query, choose 'yes'.\n",
    "    - If the documents are not relevant to the user query, choose 'no'.\n",
    "    </GUIDELINES>\n",
    "\n",
    "</SYSTEM> \n",
    "\"\"\"\n",
    "\n",
    "query_n_retrieved_docs_prompt: str = \"\"\"\n",
    "<QUERY>{query}</QUERY>\n",
    "<RETRIEVED_DOCUMENTS>{retrieved_documents}</RETRIEVED_DOCUMENTS>\n",
    "\n",
    "Are the retrieved documents relevant to the user query?\n",
    "\"\"\"\n",
    "\n",
    "query_n_response_prompt: str = \"\"\"\n",
    "<QUERY>{query}</QUERY>\n",
    "<RESPONSE>{response}</RESPONSE>\n",
    "\n",
    "Is the response relevant to the user query?\n",
    "\"\"\"\n",
    "\n",
    "rag_response_generator_prompt: str = \"\"\"\n",
    "<ROLE>\n",
    "    You're an expert at generating accurate and concise answers to user queries based on retrieved documents.\n",
    "</ROLE>\n",
    "\n",
    "    <QUERY>{query}</QUERY>\n",
    "    <RETRIEVED_DOCUMENTS>{retrieved_documents}</RETRIEVED_DOCUMENTS>\n",
    "\n",
    "    <GUIDELINES>\n",
    "    - Limit your summary to a maximum of 5 sentences.\n",
    "    - Use only the information provided in the retrieved documents.\n",
    "    </GUIDELINES>\n",
    "\"\"\"\n",
    "\n",
    "hallucination_prompt: str = \"\"\"\n",
    "<SYSTEM>\n",
    "    <ROLE>\n",
    "        You're an expert at determining whether the generated response is accurate and relevant to the user query.\n",
    "    </ROLE>\n",
    "    <VALID_OUTPUT>{valid_output}</VALID_OUTPUT>\n",
    "\n",
    "    <GUIDELINES>\n",
    "    - If the response is NOT relevant to the user query, choose 'yes'.\n",
    "    - If the response is relevant to the user query, choose 'no'.\n",
    "    </GUIDELINES>\n",
    "\n",
    "</SYSTEM> \n",
    "\"\"\"\n",
    "\n",
    "query_rewriter_prompt: str = \"\"\"\n",
    "<ROLE>\n",
    "    You're an expert at rewriting user queries to improve vector search retrieval.\n",
    "</ROLE>\n",
    "\n",
    "<ORIGINAL_QUERY>{original_query}</ORIGINAL_QUERY>\n",
    "\n",
    "<GUIDELINES>\n",
    "- Rewrite the query to be more specific and clear.\n",
    "- Ensure the rewritten query captures the user's intent accurately.\n",
    "- There must be no preamble, just the single rewritten query.\n",
    "</GUIDELINES>\n",
    "\"\"\"\n",
    "\n",
    "websearch_prompt: str = \"\"\"\n",
    "<SYSTEM>\n",
    "    <ROLE>\n",
    "        You are an expert assistant specialized in generating a concise summary of web search results.\n",
    "    </ROLE>\n",
    "\n",
    "    <GUIDELINES>\n",
    "    - Summarize the search results accurately and concisely.\n",
    "    - Limit your summary to a maximum of 5 sentences.\n",
    "    </GUIDELINES>\n",
    "\n",
    "</SYSTEM>\n",
    "\"\"\"\n",
    "\n",
    "vectorstore_routing_prompt = \"\"\"\n",
    "<INSTR>\n",
    "    Analyze this query and determine which retriever to use.\n",
    "    <QUERY>{query}</QUERY>\n",
    "</INSTR>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a18051",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f741131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# ============================= TOOLS ==============================\n",
    "# ==================================================================\n",
    "@tool(response_format=\"content\")\n",
    "async def search_tool(query: str, max_chars: int = 500) -> str:\n",
    "    \"\"\"Perform a search using TavilySearch tool.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    query: str\n",
    "        The search query.\n",
    "    max_chars: int, default=500\n",
    "        The maximum number of characters per source to return from the search results.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The formatted search results.\n",
    "    \"\"\"\n",
    "    separator: str = \"\\n\\n\"\n",
    "\n",
    "    tavily_search = TavilySearch(\n",
    "        api_key=settings.TAVILY_API_KEY.get_secret_value(),\n",
    "        max_results=3,\n",
    "        topic=\"general\",\n",
    "    )\n",
    "    search_response = await tavily_search.ainvoke({\"query\": query})\n",
    "    formatted_results: str = \"\\n\\n\".join(\n",
    "        f\"Title: {result['title']}\\nContent: {result['content'][:max_chars]} [truncated]\\nURL: {result['url']}{separator}\"\n",
    "        for result in search_response[\"results\"]\n",
    "    )\n",
    "    return formatted_results\n",
    "\n",
    "\n",
    "tool_names = [\"retriever_ai_tool\", \"retriever_football_tool\"]\n",
    "tool_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf20be",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Define Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore_football.as_retriever(search_kwargs={\"k\": 5}).invoke(\"Any news about Caicedo's contract situation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620ee9f",
   "metadata": {},
   "source": [
    "### Structured Output\n",
    "\n",
    "- For structured output, I decided to use `Instructor` because it performs better than Langchain's built-in Pydantic output parser in my tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ee4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from langchain_core.messages import AIMessage\n",
    "from langsmith import traceable\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "_async_client = AsyncOpenAI(\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    ")\n",
    "\n",
    "aclient = instructor.from_openai(\n",
    "    _async_client, mode=instructor.Mode.OPENROUTER_STRUCTURED_OUTPUTS\n",
    ")\n",
    "\n",
    "type PydanticModel = type[BaseModel]\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def get_structured_output(\n",
    "    messages: list[dict[str, Any]],\n",
    "    model: RemoteModel,\n",
    "    schema: PydanticModel,\n",
    ") -> BaseModel:\n",
    "    \"\"\"\n",
    "    Retrieves structured output from a chat completion model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    messages : list[dict[str, Any]]\n",
    "        The list of messages to send to the model for the chat completion.\n",
    "    model : RemoteModel\n",
    "        The remote model to use for the chat completion (e.g., 'gpt-4o').\n",
    "    schema : PydanticModel\n",
    "        The Pydantic schema to enforce for the structured output.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BaseModel\n",
    "        An instance of the provided Pydantic schema containing the structured output.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This is an asynchronous function that awaits the completion of the API call.\n",
    "    \"\"\"\n",
    "    return await aclient.chat.completions.create(\n",
    "        model=model,\n",
    "        response_model=schema,\n",
    "        messages=messages,\n",
    "        max_retries=5,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_langchain_messages_to_dicts(\n",
    "    messages: list[HumanMessage | SystemMessage | AIMessage],\n",
    ") -> list[dict[str, str]]:\n",
    "    \"\"\"Convert LangChain messages to a list of dictionaries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    messages : list[HumanMessage | SystemMessage | AIMessage]\n",
    "        List of LangChain message objects to convert.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, str]]\n",
    "        List of dictionaries with 'role' and 'content' keys.\n",
    "        Roles are mapped as follows:\n",
    "        - HumanMessage -> \"user\"\n",
    "        - SystemMessage -> \"system\"\n",
    "        - AIMessage -> \"assistant\"\n",
    "\n",
    "    \"\"\"\n",
    "    role_mapping: dict[str, str] = {\n",
    "        \"SystemMessage\": \"system\",\n",
    "        \"HumanMessage\": \"user\",\n",
    "        \"AIMessage\": \"assistant\",\n",
    "    }\n",
    "\n",
    "    converted_messages: list[dict[str, str]] = []\n",
    "    for msg in messages:\n",
    "        message_type: str = msg.__class__.__name__\n",
    "        role: str = role_mapping.get(\n",
    "            message_type, \"user\"\n",
    "        )  # Default to \"user\" if unknown\n",
    "        converted_messages.append({\"role\": role, \"content\": msg.content})\n",
    "\n",
    "    return converted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ccab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    fullname: str\n",
    "    salary: float\n",
    "    exeprience: int\n",
    "\n",
    "\n",
    "await get_structured_output(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Neidu Emmanuel, earning 30,000 has 4 years of experience\",\n",
    "        }\n",
    "    ],\n",
    "    model=RemoteModel.LLAMA_3_3_70B_INSTRUCT,\n",
    "    schema=Person,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74c169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424233bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator as op\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# ================== CUSTOM REDUCER FOR DICT =======================\n",
    "# ==================================================================\n",
    "def merge_dicts(existing: dict[str, Any], new: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Merge two dictionaries, with new values updating existing ones.\"\"\"\n",
    "    if existing is None:\n",
    "        return new\n",
    "    # Update existing dict with new dict values\n",
    "    return {**existing, **new}\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# ============================= STATE ==============================\n",
    "# ==================================================================\n",
    "class OtherInfo(TypedDict):\n",
    "    source_type: str\n",
    "    retrieval_relevance: str\n",
    "    is_hallucinating: str\n",
    "    rewritten_query: str\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    messages: Annotated[list[str], op.add]\n",
    "    runs: int\n",
    "    other_info: Annotated[dict[str, Any], merge_dicts]  # Use custom merger\n",
    "    documents: list[Document]\n",
    "    response: str\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# ============================= NODES ==============================\n",
    "# ==================================================================\n",
    "classifier_model: RemoteModel = RemoteModel.GPT_OSS_20B\n",
    "\n",
    "\n",
    "async def classify_query_node(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Classify the user query to determine the data source to use.\"\"\"\n",
    "    print(\"Calling ===> classify_query_node <===\")\n",
    "\n",
    "    query = state.get(\"query\")\n",
    "    sys_msg = SystemMessage(content=query_analysis_prompt.format(topics=topics))\n",
    "    messages = convert_langchain_messages_to_dicts(\n",
    "        [sys_msg, HumanMessage(content=query)]\n",
    "    )\n",
    "    query_type: RouteQuerySchema = await get_structured_output(\n",
    "        messages=messages,\n",
    "        model=classifier_model,\n",
    "        schema=RouteQuerySchema,\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Classified query to use data source: {query_type.data_source.value}\")\n",
    "    return {\"other_info\": {\"source_type\": query_type.data_source.value}}\n",
    "\n",
    "\n",
    "async def llm_call_node(state: State) -> dict[str, Any]:\n",
    "    print(\"Calling ===> llm_call_node <===\")\n",
    "\n",
    "    query = state.get(\"query\")\n",
    "    if not query and \"messages\" in state:\n",
    "        messages = state[\"messages\"]\n",
    "        query = messages[-1] if isinstance(messages, list) else messages\n",
    "\n",
    "    llm_with_tools = remote_llm.bind_tools([search_tool])\n",
    "    response = await llm_with_tools.ainvoke(query)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        # Messages key is the default key for tools\n",
    "        \"messages\": [response],\n",
    "    }\n",
    "\n",
    "\n",
    "async def generate_web_search_response(state: State) -> dict[str, Any]:\n",
    "    print(\"Calling ===> generate_web_search_response <===\")\n",
    "\n",
    "    message: str = state.get(\"messages\", [])[-1].content\n",
    "    if not message:\n",
    "        return {\n",
    "            \"response\": \"I couldn't find relevant information to answer your query.\"\n",
    "        }\n",
    "    sys_msg = SystemMessage(content=websearch_prompt)\n",
    "    prompt: str = f\"SEARCH RESULTS:\\n{message}\"\n",
    "\n",
    "    response = await remote_llm.ainvoke([sys_msg, HumanMessage(content=prompt)])\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response.content,\n",
    "    }\n",
    "\n",
    "\n",
    "async def retrieve_documents(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Retrieve documents by intelligently selecting the appropriate retriever.\"\"\"\n",
    "    max_chars: int = 1_000\n",
    "    print(\"Calling ===> retrieve_documents <===\")\n",
    "\n",
    "    query = state.get(\"query\")\n",
    "    prompt: str = vectorstore_routing_prompt.format(query=query)\n",
    "\n",
    "    user_msg = {\"role\": \"user\", \"content\": prompt}\n",
    "\n",
    "    retriever_choice = await get_structured_output(\n",
    "        messages=[user_msg],\n",
    "        model=classifier_model,\n",
    "        schema=VectorSearchTypeSchema,\n",
    "    )\n",
    "    retriever_choice: str = retriever_choice.vector_search_type.value\n",
    "\n",
    "    print(f\"✅ Retriever choice: {retriever_choice}\")\n",
    "\n",
    "    # Retrieve documents based on the routing decision\n",
    "    if retriever_choice == VectorSearchType.FOOTBALL.value:\n",
    "        retrieved_docs = vectorstore_football.similarity_search(query, k=3)\n",
    "        print(f\"✅ Used football retriever, found {len(retrieved_docs)} documents\")\n",
    "    elif retriever_choice == VectorSearchType.AI.value:\n",
    "        retrieved_docs = vectorstore_ai.similarity_search(query, k=3)\n",
    "        print(f\"✅ Used AI retriever, found {len(retrieved_docs)} documents\")\n",
    "    else:\n",
    "        return {\"response\": \"I couldn't find the vectorstore to answer your query.\"}\n",
    "\n",
    "    # Format documents for message display\n",
    "    formatted_docs = \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata.get('source', 'Unknown')}\\nContent: {doc.page_content[:max_chars]} [truncated]\\n\"\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"documents\": retrieved_docs,\n",
    "        \"messages\": [f\"Retrieved {len(retrieved_docs)} documents:\\n{formatted_docs}\"],\n",
    "    }\n",
    "\n",
    "\n",
    "async def grade_documents(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Grade the relevance of retrieved documents.\"\"\"\n",
    "    print(\"Calling ===> grade_documents <===\")\n",
    "\n",
    "    query = state.get(\"query\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    if not documents:\n",
    "        print(\"⚠️ No documents to grade\")\n",
    "        return {\"other_info\": {\"retrieval_relevance\": YesOrNo.NO.value}}\n",
    "\n",
    "    # Grade each document\n",
    "    relevant_docs: list[Document] = []\n",
    "    for doc in documents:\n",
    "        doc_content = f\"Source: {doc.metadata}\\nContent: {doc.page_content}\"\n",
    "\n",
    "        sys_msg = SystemMessage(\n",
    "            content=retrieval_grading_prompt.format(\n",
    "                valid_output=valid_output, retrieved_documents=doc_content\n",
    "            )\n",
    "        )\n",
    "        grading_query = query_n_retrieved_docs_prompt.format(\n",
    "            query=query, retrieved_documents=doc_content\n",
    "        )\n",
    "\n",
    "        messages = convert_langchain_messages_to_dicts(\n",
    "            [sys_msg, HumanMessage(content=grading_query)]\n",
    "        )\n",
    "        grade: GradeRetrievalSchema = await get_structured_output(\n",
    "            messages=messages,\n",
    "            model=classifier_model,\n",
    "            schema=GradeRetrievalSchema,\n",
    "        )\n",
    "\n",
    "        if grade.is_relevant.value == YesOrNo.YES.value:\n",
    "            relevant_docs.append(doc)\n",
    "\n",
    "    print(f\"✅ Graded documents: {len(relevant_docs)}/{len(documents)} relevant\")\n",
    "\n",
    "    return {\n",
    "        \"documents\": relevant_docs,\n",
    "        \"other_info\": {\n",
    "            \"retrieval_relevance\": (\n",
    "                YesOrNo.YES.value if relevant_docs else YesOrNo.NO.value\n",
    "            )\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue_to_retrieve(state: State) -> Literal[\"retrieve\", \"web_search\"]:\n",
    "    source_type = state.get(\"other_info\", {}).get(\"source_type\", DataSource.WEBSEARCH)\n",
    "\n",
    "    if source_type == DataSource.VECTORSTORE.value:\n",
    "        return \"retrieve\"\n",
    "    return \"web_search\"\n",
    "\n",
    "\n",
    "def should_continue_to_generate(\n",
    "    state: State,\n",
    ") -> Literal[\"generate\", \"rewrite\", \"failed\"]:  # type: ignore\n",
    "    relevance = state.get(\"other_info\", {}).get(\"retrieval_relevance\", YesOrNo.NO)\n",
    "    runs: int = state.get(\"runs\", 0)\n",
    "\n",
    "    if runs <= 3:\n",
    "        if relevance == YesOrNo.YES.value:\n",
    "            return \"generate\"\n",
    "        return \"rewrite\"\n",
    "\n",
    "    return \"failed\"\n",
    "\n",
    "\n",
    "async def generate_response(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Generate response based on retrieved documents.\"\"\"\n",
    "    print(\"Calling ===> generate_response <===\")\n",
    "\n",
    "    query = state.get(\"query\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    if not documents:\n",
    "        return {\n",
    "            \"response\": \"I couldn't find relevant information to answer your query.\"\n",
    "        }\n",
    "\n",
    "    if documents:\n",
    "        # Format documents for the prompt\n",
    "        formatted_docs = \"\\n\\n\".join(\n",
    "            f\"Document {i + 1}:\\n{doc.page_content}\" for i, doc in enumerate(documents)\n",
    "        )\n",
    "\n",
    "    prompt = rag_response_generator_prompt.format(\n",
    "        query=query, retrieved_documents=formatted_docs\n",
    "    )\n",
    "\n",
    "    response = await remote_llm.ainvoke(prompt)\n",
    "\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "\n",
    "async def check_hallucination_node(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Check if the generated response contains hallucinations.\"\"\"\n",
    "    print(\"Calling ===> check_hallucination_node <===\")\n",
    "\n",
    "    query = state.get(\"query\")\n",
    "    runs: int = state.get(\"runs\", 0)\n",
    "\n",
    "    response = state.get(\"response\")\n",
    "\n",
    "    sys_msg = SystemMessage(\n",
    "        content=hallucination_prompt.format(valid_output=valid_output)\n",
    "    )\n",
    "    check_query = query_n_response_prompt.format(query=query, response=response)\n",
    "\n",
    "    messages = convert_langchain_messages_to_dicts(\n",
    "        [sys_msg, HumanMessage(content=check_query)]\n",
    "    )\n",
    "    result: HallucinationSchema = await get_structured_output(\n",
    "        messages=messages,\n",
    "        model=classifier_model,\n",
    "        schema=HallucinationSchema,\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Hallucination check: {result.is_hallucinating.value}\")\n",
    "\n",
    "    return {\n",
    "        \"runs\": runs + 1,\n",
    "        \"other_info\": {\"is_hallucinating\": result.is_hallucinating.value},\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue_to_final_answer(\n",
    "    state: State,\n",
    ") -> Literal[\"answer\", \"rewrite\", \"failed\"]:  # type: ignore\n",
    "    is_hallucinating = state.get(\"other_info\", {}).get(\"is_hallucinating\", YesOrNo.YES)\n",
    "    runs: int = state.get(\"runs\", 0)\n",
    "\n",
    "    if runs <= 3:\n",
    "        if is_hallucinating == YesOrNo.NO.value:\n",
    "            return \"answer\"\n",
    "        return \"rewrite\"\n",
    "\n",
    "    return \"failed\"\n",
    "\n",
    "\n",
    "async def rewrite_query(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Rewrite the query to improve retrieval.\"\"\"\n",
    "    print(\"Calling ===> rewrite_query <===\")\n",
    "    runs: int = state.get(\"runs\", 0)\n",
    "\n",
    "    query = state.get(\"query\")\n",
    "    prompt = query_rewriter_prompt.format(original_query=query)\n",
    "    response = await remote_llm.ainvoke(prompt)\n",
    "\n",
    "    rewritten = response.content\n",
    "    print(f\"Original: {query}\\nRewritten: {rewritten}\")\n",
    "    print(f\"⚠️ Runs: {runs + 1}\")\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"runs\": runs + 1,\n",
    "        \"other_info\": {\"rewritten_query\": rewritten},\n",
    "    }\n",
    "\n",
    "\n",
    "def failed_node(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Finalize the answer.\"\"\"\n",
    "    print(\"Calling ===> failed_node <===\")\n",
    "\n",
    "    return {\n",
    "        \"response\": state.get(\n",
    "            \"response\", \"I couldn't find relevant information to answer your query.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def answer_node(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Finalize the answer.\"\"\"\n",
    "    print(\"Calling ===> answer_node <===\")\n",
    "\n",
    "    response = state.get(\n",
    "        \"response\", \"I couldn't find relevant information to answer your query.\"\n",
    "    )\n",
    "\n",
    "    return {\"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, Markdown, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import RetryPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18913adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.prebuilt import ToolNode, tools_condition\n",
    "# from langgraph.types import RetryPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a39830",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder: StateGraph = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "tool_node = ToolNode([search_tool])\n",
    "\n",
    "builder.add_node(\n",
    "    \"query_analysis\",\n",
    "    classify_query_node,\n",
    "    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0),\n",
    ")\n",
    "builder.add_node(\n",
    "    \"tools\", tool_node, retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)\n",
    ")\n",
    "builder.add_node(\"llm_call_node\", llm_call_node)\n",
    "builder.add_node(\"retrieve\", retrieve_documents)\n",
    "builder.add_node(\n",
    "    \"grade\",\n",
    "    grade_documents,\n",
    "    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0),\n",
    ")\n",
    "builder.add_node(\n",
    "    \"generate\",\n",
    "    generate_response,\n",
    "    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0),\n",
    ")\n",
    "builder.add_node(\n",
    "    \"rewrite\",\n",
    "    rewrite_query,\n",
    "    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0),\n",
    ")\n",
    "builder.add_node(\n",
    "    \"check_hallucination\",\n",
    "    check_hallucination_node,\n",
    "    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0),\n",
    ")\n",
    "builder.add_node(\n",
    "    \"web_search_response\",\n",
    "    generate_web_search_response,\n",
    "    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0),\n",
    ")\n",
    "builder.add_node(\"answer\", answer_node)\n",
    "builder.add_node(\"failed\", failed_node)\n",
    "\n",
    "\n",
    "# Build the graph query_analysis\n",
    "builder.add_edge(START, \"query_analysis\")\n",
    "builder.add_conditional_edges(\n",
    "    \"query_analysis\",\n",
    "    should_continue_to_retrieve,\n",
    "    {\"retrieve\": \"retrieve\", \"web_search\": \"llm_call_node\"},\n",
    ")\n",
    "builder.add_conditional_edges(\n",
    "    \"llm_call_node\",\n",
    "    tools_condition,\n",
    "    {\"tools\": \"tools\", END: \"failed\"},\n",
    ")\n",
    "builder.add_edge(\"tools\", \"web_search_response\")\n",
    "builder.add_edge(\"web_search_response\", END)\n",
    "builder.add_edge(\"retrieve\", \"grade\")\n",
    "builder.add_conditional_edges(\n",
    "    \"grade\",\n",
    "    should_continue_to_generate,\n",
    "    {\"generate\": \"generate\", \"rewrite\": \"rewrite\", \"failed\": \"failed\"},\n",
    ")\n",
    "builder.add_edge(\"generate\", \"check_hallucination\")\n",
    "builder.add_conditional_edges(\n",
    "    \"check_hallucination\",\n",
    "    should_continue_to_final_answer,\n",
    "    {\"answer\": \"answer\", \"rewrite\": \"rewrite\", \"failed\": \"failed\"},\n",
    ")\n",
    "builder.add_edge(\"answer\", END)\n",
    "builder.add_edge(\"rewrite\", \"retrieve\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "# Visualize the graph with ASCII fallback\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    console.print(f\"[yellow]PNG visualization failed: {e}[/yellow]\")\n",
    "    console.print(\"[cyan]Displaying ASCII representation instead:[/cyan]\\n\")\n",
    "    try:\n",
    "        print(graph.get_graph(xray=1).draw_ascii())\n",
    "    except ImportError as ie:\n",
    "        console.print(f\"[red]ASCII visualization also failed: {ie}[/red]\")\n",
    "        console.print(\"[magenta]Showing basic graph structure:[/magenta]\\n\")\n",
    "        graph_obj = graph.get_graph(xray=1)\n",
    "        console.print(f\"Nodes: {[node.id for node in graph_obj.nodes.values()]}\")\n",
    "        console.print(f\"Edges: {[(e.source, e.target) for e in graph_obj.edges]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d13ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_llm = ChatOpenAI(\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),  # type: ignore\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    model=RemoteModel.GPT_OSS_20B,\n",
    ")\n",
    "\n",
    "\n",
    "# Test the LLMs\n",
    "response = remote_llm.invoke(\"Tell me a very short joke.\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089bd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-build the graph\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "config: dict[str, Any] = {\"configurable\": {\"thread_id\": \"test-01\"}}\n",
    "response = await graph.ainvoke(\n",
    "    {\"query\": \"Any news on Liverpool's player meetings?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27050883",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"### Final Response:\\n\\n{response['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b6df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1159488",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_up_from_current_directory(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50f95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35c47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-rag (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
